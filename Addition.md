Основной код был взять из репозитория и практической части занятия.
Что было протестировано:
1) Балансировка тренировочной выборки с помощью отбрасывания элементов после определенного количества
2) Уменьшение LR
3) Добавление четвертого сверточного слоя в модель
4) Изменение значения Dropout

# Балансировка тренировочной выборки
Изначально количество изображений каждого из классов не было сбалансировано. Дисбаланс классов 2(340, volleyball) и 3(646, golf) почти в 2 раза. 
- Попробовал сделать одинаковым количество элементов каждого из классов

Брал в каждом классе тренировочного датасета по 340 первых элементов.

График функции потерь резко менял значания, в основном на валидации loss был больше.

После того как вернул все элементы в тренировочную выборку loss стал более стабильным - отклонение значение от предыдущего стало меньше, а сами значения находились в окрестности loss тренировочного

# Уменьшение LR
В расчете на сглаживание графика loss уменьшил LR с 0.0005 до 0.0003. Однако это только усилило колебания графика loss. В среднем значения стали больше чем на тренировке. 

Оставил LR 0.0005

# Добавление четвертого сверточного слоя
После добавления четвертого слоя loss вновь стал более стабильным, но есть подозрение по поведению графиков loss, что начинается переобучение, однако метрики в среднем растут

# Изменение Dropout
При изменении Dropout до 0.35 (с 0.3) loss функций расходился и метрика f1 в основном не росла, скорее уменьшалась. Максимальное значение было порядка 0.5.

Попробовал Dropout 0.25 и уменьшить LR до 0.0002. Графики loss начинали расходиться на 10 эпох позже и f1 стала достигать 0.55-0.6

Максимальное значение f1 достигло 0.63 при примерно 65-57 эпохе и составило 0.63 (скрин не сохранился). После этого было проведено еще 10 эпох для той же модели и f1 вновь достигло 0.635, но лучше результата достичь этой модели не удалось

# Две модели
Взяв идею из практического занятия продолжил исследование той же модели после 70 эпохи, но в этот раз уменьшив LR до 0.0001. Результатом было улучшение f1 минимум до 0.64 с максимальным значением 0.6593 на 12 эпохе после первых 70, но графики loss сходиться так и не начали

# Без скриншотов
Пробовал менять вероятности аугментации. Большой разницы не увидел. Знаю, что на самом первом скриншоте обучение не с самого начала, при написании пояснения обнаружил, но не перескринил, результат все равно будет аналогичный

# Трудности
- Библиотека для стопера не захотела работать, устанавливать через pip пробовал, но все равно не помогло.
- Не понял почему графики Loss расходятся. Кажется переобучение, но метрики улучшаются

# Лучший результат по метрикам 
LR=0.0002+0.0001
EPOCH=70+4

Accuracy=0.65779
Precision=0.67702
Recall=0.653878
F1=0.659232